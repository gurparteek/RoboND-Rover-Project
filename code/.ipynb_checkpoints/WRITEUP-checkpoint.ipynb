{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Describe in your writeup (and identify where in your code) how you modified or added functions to add obstacle and rock sample identification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Answer 1: Under Color Thresholding, three new functions were added:\n",
    "#For identifying the navigable path:\n",
    "def find_nav_path(img, thresh=[160,160,160]):\n",
    "    # Create an array of zeros same xy size as img, but single channel\n",
    "    masked_img = np.zeros_like(img[:,:,0])\n",
    "    #Creating a mask for assigning 1 or True, to the pixels that satisfy all three thresholds.\n",
    "    #All three need to be true, so bitwise & for the single bit arrays.\n",
    "    mask = (img[:,:,0] > thresh[0]) & (img[:,:,1] > thresh[1]) & (img[:,:,2] > thresh[2])\n",
    "    #Item assignemt of True through masking to the pixels that satify the above condition.\n",
    "    masked_img[mask] = 1\n",
    "    # Return the binary image\n",
    "    return masked_img\n",
    "\n",
    "#In a similar fashion, create two more functions for identifying obstacles and the rocks.\n",
    "\n",
    "#Identifying the rocks.\n",
    "#R and G make yellow so low-cap the intensity for them at 115 and high-cap the intensity for B at 100.\n",
    "def find_rocks(img, thresh = [115,115,100]):\n",
    "    masked_img = np.zeros_like(img[:,:,0])\n",
    "    mask = (img[:,:,0] > thresh[0]) & (img[:,:,1] > thresh[1]) & (img[:,:,2] < thresh[2])\n",
    "    masked_img[mask] = 1\n",
    "    return masked_img\n",
    "\n",
    "#Identifying the obstacles.\n",
    "#Threshold of RGB < 160 reverses the threshold to find the obstacles.\n",
    "def find_obstacles(img, thresh = [150,150,150]):\n",
    "    masked_img = np.zeros_like(img[:,:,0])\n",
    "    mask = (img[:,:,0] < thresh[0]) & (img[:,:,1] < thresh[1]) & (img[:,:,2] < thresh[2])\n",
    "    masked_img[mask] = 1\n",
    "    return masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Describe in your writeup how you modified the process_image() to demonstrate your analysis and how you created a worldmap. Include your video output with your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    # 1) Defining the source and destination points for perspective transform\n",
    "    dst_size = 5 \n",
    "    bottom_offset = 6\n",
    "    source = np.float32([[14, 140], [301 ,140],[200, 96], [118, 96]])\n",
    "    destination = np.float32([[img.shape[1]/2 - dst_size, img.shape[0] - bottom_offset],\n",
    "                      [img.shape[1]/2 + dst_size, img.shape[0] - bottom_offset],\n",
    "                      [img.shape[1]/2 + dst_size, img.shape[0] - 2*dst_size - bottom_offset], \n",
    "                      [img.shape[1]/2 - dst_size, img.shape[0] - 2*dst_size - bottom_offset],\n",
    "                      ])\n",
    "    \n",
    "    # 2) Applying the perspective transform\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "    \n",
    "    # 3) Applying the color threshold to identify navigable terrain/obstacles/rock samples\n",
    "    nav_path_img = find_nav_path(warped)\n",
    "    rocks_img = find_rocks(warped)\n",
    "    obstacles_img = find_obstacles(warped)\n",
    "    \n",
    "    # 4) Converting thresholded image pixel values to rover-centric coords\n",
    "    nav_x, nav_y = rover_coords(nav_path_img)\n",
    "    rock_x, rock_y = rover_coords(rocks_img)\n",
    "    obstacle_x, obstacle_y = rover_coords(obstacles_img)\n",
    "    \n",
    "    # 5) Converting rover-centric pixel values to world coords\n",
    "    xpos = data.xpos[data.count]\n",
    "    ypos = data.ypos[data.count]\n",
    "    yaw = data.yaw[data.count]\n",
    "    world_size = data.worldmap.shape[0]\n",
    "    scale = 10\n",
    "    \n",
    "    nav_x_world, nav_y_world = pix_to_world(nav_x, nav_y, xpos, ypos, yaw, world_size, scale)\n",
    "    rock_x_world, rock_y_world = pix_to_world(rock_x, rock_y, xpos, ypos, yaw, world_size, scale)\n",
    "    obstacle_x_world, obstacle_y_world = pix_to_world(obstacle_x, obstacle_y, xpos, ypos, yaw, world_size, scale)\n",
    "    \n",
    "    # 6) Updating the worldmap\n",
    "    data.worldmap[obstacle_y_world, obstacle_x_world, 0] += 1\n",
    "    data.worldmap[rock_y_world, rock_x_world, 1] += 1\n",
    "    data.worldmap[nav_y_world, nav_x_world, 2] += 1\n",
    "\n",
    "    # 7) Making the mosaic image\n",
    "    output_image = np.zeros((img.shape[0] + data.worldmap.shape[0], img.shape[1]*2, 3))\n",
    "    output_image[0:img.shape[0], 0:img.shape[1]] = img\n",
    "\n",
    "    warped = perspect_transform(img, source, destination)\n",
    "        # Add the warped image in the upper right hand corner\n",
    "    output_image[0:img.shape[0], img.shape[1]:] = warped\n",
    "\n",
    "        # Overlay worldmap with ground truth map\n",
    "    map_add = cv2.addWeighted(data.worldmap, 1, data.ground_truth, 0.5, 0)\n",
    "        # Flip map overlay so y-axis points upward and add to output_image \n",
    "    output_image[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)\n",
    "\n",
    "    cv2.putText(output_image,\"Populate this image with your analyses to make a video!\", (20, 20), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1)\n",
    "    data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "###NOTE: Output video in the output folder please."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. perception_step() and decision_step() functions have been filled in and their functionality explained in the writeup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
